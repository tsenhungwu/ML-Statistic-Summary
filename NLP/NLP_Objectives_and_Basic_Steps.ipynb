{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Objectives and Basic Steps\n",
    "- Objectives\n",
    "   * Split documents into tokens or segments\n",
    "   * Let machine understand and have the ability to read by feature extraction\n",
    "   \n",
    "- Common Terminology\n",
    "    * Corpus\n",
    "    * Document\n",
    "    * Token\n",
    "    * Bag of Words\n",
    "    * Vocab\n",
    "    * Stop Words\n",
    "    \n",
    "##  1. Basic processing steps\n",
    "   * Read in documents (corpus).\n",
    "   * Tokenization & Lower case: split documents into individual words or segments.\n",
    "   * Strip out spacing and punctuation.\n",
    "   * Remove stop words.\n",
    "   * Stemming & Lemmatization.\n",
    "   * Bag of words: document-to-term matrix.\n",
    "   * Term Frequency and Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "## 2. Document searching - by similarity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = 'Wise people think they are foolish.'\n",
    "doc2 = 'Foolish foolish people think they are wise wise.'\n",
    "doc3 = 'I am definitely wise so this irritates me.'\n",
    "doc4 = 'ABC is for sure like definitely foolish.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Corpus\n",
    "- Corpus means a list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc1, doc2, doc3, doc4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenization and Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['wise', 'people', 'think', 'they', 'are', 'foolish.']\n",
      "Tokens: ['foolish', 'foolish', 'people', 'think', 'they', 'are', 'wise', 'wise.']\n",
      "Tokens: ['i', 'am', 'definitely', 'wise', 'so', 'this', 'irritates', 'me.']\n",
      "Tokens: ['abc', 'is', 'for', 'sure', 'like', 'definitely', 'foolish.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization: without any package.\n",
    "for doc in corpus:\n",
    "    lower_doc = doc.lower()\n",
    "    tokens = lower_doc.split(' ')\n",
    "    print(\"Tokens: {}\".format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wise', 'people', 'think', 'they', 'are', 'foolish', '.'],\n",
       " ['foolish', 'foolish', 'people', 'think', 'they', 'are', 'wise', 'wise', '.'],\n",
       " ['i', 'am', 'definitely', 'wise', 'so', 'this', 'irritates', 'me', '.'],\n",
       " ['abc', 'is', 'for', 'sure', 'like', 'definitely', 'foolish', '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization: with nltk package\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "[word_tokenize(doc.lower()) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Strip out spacing and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wise', 'people', 'think', 'they', 'are', 'foolish', '.'],\n",
       " ['foolish', 'foolish', 'people', 'think', 'they', 'are', 'wise', 'wise', '.'],\n",
       " ['i', 'am', 'definitely', 'wise', 'so', 'this', 'irritates', 'me', '.'],\n",
       " ['abc', 'is', 'for', 'sure', 'like', 'definitely', 'foolish', '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "token_ls = []\n",
    "for document in tokenized_corpus:\n",
    "    tokens = []\n",
    "    for token in document:\n",
    "        if token.strip(string.punctuation) != '':\n",
    "            tokens.append(token.strip(string.punctuation))\n",
    "    token_ls.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wise', 'people', 'think', 'they', 'are', 'foolish'],\n",
       " ['foolish', 'foolish', 'people', 'think', 'they', 'are', 'wise', 'wise'],\n",
       " ['i', 'am', 'definitely', 'wise', 'so', 'this', 'irritates', 'me'],\n",
       " ['abc', 'is', 'for', 'sure', 'like', 'definitely', 'foolish']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', 'did', 'does', 'doesn', 'herself', 'yours', 'through', 'ours', 'shouldn', 've', 'by', 'above', 'was', 'haven', \"haven't\", 'just', 'during', 'the', 'hadn', 'we', \"you'd\", 'theirs', 'wouldn', 'again', 'our', 'has', 'before', 'at', 'mightn', 'those', 't', 'now', 'them', 'couldn', 'under', \"mustn't\", 'who', 'in', 'only', 'o', 'where', 'own', 'same', 'too', 'while', 'be', 'yourselves', 'of', 'about', 'on', 'any', 'hasn', 'can', 'shan', \"it's\", \"shouldn't\", 'd', \"aren't\", 'some', 'wasn', 'nor', 'themselves', \"wasn't\", 'an', 'this', 'am', 'over', 're', 'so', \"shan't\", 'had', 'but', 'when', 'that', 'she', \"won't\", 'to', 'isn', 'off', 'been', 'mustn', 'him', 'than', 'against', 'not', 'further', \"weren't\", 'which', 'each', 'y', 'such', 's', 'my', 'being', 'up', 'he', 'needn', 'himself', 'below', \"hadn't\", \"doesn't\", 'hers', \"isn't\", \"mightn't\", 'for', 'out', 'very', 'his', 'these', 'do', 'll', \"couldn't\", 'all', 'because', 'you', 'have', 'there', 'weren', 'ourselves', 'after', 'ma', \"hasn't\", \"she's\", 'few', 'more', 'is', 'and', \"you're\", \"don't\", 'didn', 'once', 'having', 'between', 'into', 'will', 'most', 'me', 'here', 'their', 'its', 'itself', \"you've\", \"should've\", 'if', 'other', 'won', 'they', 'are', 'your', 'down', 'how', 'should', \"you'll\", 'aren', 'no', 'why', 'or', \"wouldn't\", 'then', 'i', 'yourself', 'both', 'myself', \"didn't\", \"needn't\", 'her', 'doing', 'a', 'as', 'from', 'ain', 'until', 'it', 'what', \"that'll\", 'm', 'were', 'with', 'whom'}\n",
      "\n",
      "179 stop words.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "print()\n",
    "print('{} stop words.'.format(len(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words.\n",
    "token_ls = [[token for token in document if token not in stop_words] \n",
    "        for document in token_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wise', 'people', 'think', 'foolish'],\n",
       " ['foolish', 'foolish', 'people', 'think', 'wise', 'wise'],\n",
       " ['definitely', 'wise', 'irritates'],\n",
       " ['abc', 'sure', 'like', 'definitely', 'foolish']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Stemming & Lemmatization\n",
    "- Stemming: reducing inflected (or sometimes derived) words to their stem, base or root form.\n",
    "- Lemmatization: determining the lemma for a given word, \n",
    "   * A lemma is a word which stands at the head of a definition in a dictionary, e.g. run (lemma),  runs, ran and running (inflections) .\n",
    "   * Lemmatization is a complex task involving understanding context and determining the part of speech of a word in a sentence.\n",
    "      * e.g. \"organized\" (verb or adjective?)\n",
    "   * The widely used Lemmatization method is based on WordNet, a large lexical database of English.\n",
    "- Difference between Stemming & Lemmatization\n",
    "   * A stemmer operates on a single word **without knowledge of the context**, and therefore cannot discriminate between words which have different meanings depending on part of speech. While, lemmatization **requires context and POS tags**. \n",
    "   * Stemming may not generate a real word, but lemmization always generates real words.\n",
    "   *  However, stemmers are typically easier to implement and run faster with reduced accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wise', 'peopl', 'think', 'foolish'],\n",
       " ['foolish', 'foolish', 'peopl', 'think', 'wise', 'wise'],\n",
       " ['definit', 'wise', 'irrit'],\n",
       " ['abc', 'sure', 'like', 'definit', 'foolish']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stem_docs = [[porter_stemmer.stem(token) for token in document]\n",
    "               for document in token_ls\n",
    "            ]\n",
    "stem_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wise', 'people', 'think', 'foolish'],\n",
       " ['foolish', 'foolish', 'people', 'think', 'wise', 'wise'],\n",
       " ['definitely', 'wise', 'irritates'],\n",
       " ['abc', 'sure', 'like', 'definitely', 'foolish']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_docs = [[wordnet_lemmatizer.lemmatize(token) for token in document]\n",
    "               for document in token_ls\n",
    "            ]\n",
    "lemma_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - Construct Vocab & use Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before getting the bag of words, we need to attain the 'Vocab'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab\n",
    "- Set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wise',\n",
       " 'people',\n",
       " 'think',\n",
       " 'foolish',\n",
       " 'foolish',\n",
       " 'foolish',\n",
       " 'people',\n",
       " 'think',\n",
       " 'wise',\n",
       " 'wise',\n",
       " 'definitely',\n",
       " 'wise',\n",
       " 'irritates',\n",
       " 'abc',\n",
       " 'sure',\n",
       " 'like',\n",
       " 'definitely',\n",
       " 'foolish']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use lemmatized documents as an example.\n",
    "# Collect all tokens in a list.\n",
    "vocabulary = [token for document in lemma_docs for token in document]\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique tokens.\n",
    "vocabulary = sorted(list(set(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (features): ['abc', 'definitely', 'foolish', 'irritates', 'like', 'people', 'sure', 'think', 'wise']\n"
     ]
    }
   ],
   "source": [
    "# vocabulary is the feature space.\n",
    "print('Vocabulary (features):', vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words\n",
    "- Document-to-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function.\n",
    "def bow_vectorize(doc, vocabulary):\n",
    "    # Counter output is a dictionary object (count on each word).\n",
    "    bag_of_words = Counter(doc)\n",
    "    \n",
    "    # Setup an empty list containing zeros given the length of vocabulary.\n",
    "    doc_vector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    # Loop over by document, if the word exists in a document, increase count value for that word\n",
    "    for word_index, word in enumerate(vocabulary):\n",
    "        if word in bag_of_words:\n",
    "            doc_vector[word_index] += bag_of_words[word]\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_matrix = list()\n",
    "for document in lemma_docs:\n",
    "    bag_of_words_matrix.append(bow_vectorize(document, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['abc', 'definitely', 'foolish', 'irritates', 'like', 'people', 'sure', 'think', 'wise']\n",
      "\"['wise', 'people', 'think', 'foolish']\": \n",
      " [0. 0. 1. 0. 0. 1. 0. 1. 1.] \n",
      "\n",
      "\"['foolish', 'foolish', 'people', 'think', 'wise', 'wise']\": \n",
      " [0. 0. 2. 0. 0. 1. 0. 1. 2.] \n",
      "\n",
      "\"['definitely', 'wise', 'irritates']\": \n",
      " [0. 1. 0. 1. 0. 0. 0. 0. 1.] \n",
      "\n",
      "\"['abc', 'sure', 'like', 'definitely', 'foolish']\": \n",
      " [1. 1. 1. 0. 1. 0. 1. 0. 0.] \n",
      "\n",
      "Feature matrix:\n",
      "[array([0., 0., 1., 0., 0., 1., 0., 1., 1.]), array([0., 0., 2., 0., 0., 1., 0., 1., 2.]), array([0., 1., 0., 1., 0., 0., 0., 0., 1.]), array([1., 1., 1., 0., 1., 0., 1., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# Check final results\n",
    "print('Features:', vocabulary)\n",
    "\n",
    "for i in range(len(bag_of_words_matrix)):\n",
    "    print('\"%s\":'% lemma_docs[i], '\\n', bag_of_words_matrix[i], '\\n')\n",
    "          \n",
    "print('Feature matrix:')\n",
    "print(bag_of_words_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NLP at one time: tokenize, lowercase, and lemmatize.\n",
    "def lemmatize(doc):\n",
    "    return [wordnet_lemmatizer.lemmatize(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix - Method 2\n",
      "[[0 0 1 0 0 1 0 1 1]\n",
      " [0 0 2 0 0 1 0 1 2]\n",
      " [0 1 0 1 0 0 0 0 1]\n",
      " [1 1 1 0 1 0 1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuchenhong/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Do NLP at one time: tokenize, lowercase, remove stop words, lemmatize, and bag of words.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=stopwords.words('english'),\n",
    "                                   vocabulary=vocabulary,\n",
    "                                   tokenizer=lemmatize)\n",
    "\n",
    "# Can perform on multiple documents.\n",
    "print('Feature matrix - Method 2')\n",
    "print(count_vectorizer.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix - Method 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 1., 0., 0., 1., 0., 1., 1.]),\n",
       " array([0., 0., 2., 0., 0., 1., 0., 1., 2.]),\n",
       " array([0., 1., 0., 1., 0., 0., 0., 0., 1.]),\n",
       " array([1., 1., 1., 0., 1., 0., 1., 0., 0.])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compared with results using Method 1\n",
    "print('Feature matrix - Method 1')\n",
    "bag_of_words_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "- How many times a word appears in a document / total words in a document. (Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vectorize(doc, vocabulary):\n",
    "    bow_vector = bow_vectorize(doc, vocabulary)\n",
    "    tf_vector = np.zeros(len(vocabulary))\n",
    "    for idx, vec in enumerate(bow_vector):\n",
    "        tf_vector[idx] = vec / len(doc)\n",
    "    return tf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = list()\n",
    "for doc in lemma_docs:\n",
    "    tf_matrix.append(tf_vectorize(doc, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.  , 0.  , 0.25, 0.  , 0.  , 0.25, 0.  , 0.25, 0.25]),\n",
       " array([0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.16666667, 0.        , 0.16666667, 0.33333333]),\n",
       " array([0.        , 0.33333333, 0.        , 0.33333333, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.33333333]),\n",
       " array([0.2, 0.2, 0.2, 0. , 0.2, 0. , 0.2, 0. , 0. ])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "- Measures how important a term is within the corpus.\n",
    "- Frequent terms assigned lower weights while rare terms assigned higher weights.\n",
    "- Let $|D|$ denote the number of documents, $df(w,D)$ denotes the number of documents with term $w$ in them. Then, $$idf(w) = ln(\\frac{|D|}{df(w,D)})+1$$ Or a smoothed version of IDF: $$idf(w) = ln(\\frac{|D|+1}{df(w,D)+1})+1$$\n",
    "- Why plus 1? To avoid a zero value occured. And, smoothed version is smaller than the normal version of IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 0, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get document frequency\n",
    "df = np.where(np.array(tf_matrix)>0,1,0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF Matrix:\n",
      "[2.38629436 1.69314718 1.28768207 2.38629436 2.38629436 1.69314718\n",
      " 2.38629436 1.69314718 1.28768207]\n",
      "\n",
      "Smoothed IDF Matrix:\n",
      "[1.91629073 1.51082562 1.22314355 1.91629073 1.91629073 1.51082562\n",
      " 1.91629073 1.51082562 1.22314355]\n"
     ]
    }
   ],
   "source": [
    "# Get document frequency.\n",
    "df = np.where(np.array(tf_matrix)>0,1,0)\n",
    "df\n",
    "\n",
    "# Get IDF matrix.\n",
    "idf = np.log(np.divide(len(corpus), np.sum(df, axis=0))) + 1\n",
    "print(\"\\nIDF Matrix:\")\n",
    "print(idf)\n",
    "\n",
    "# Smoothed version of IDF matrix.\n",
    "smoothed_idf=np.log(np.divide(len(corpus)+1, np.sum(df, axis=0)+1)) + 1\n",
    "print(\"\\nSmoothed IDF Matrix:\")\n",
    "print(smoothed_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "- Let $s(w,d)=tf(w,d) * idf(w)$, normalize the TF-IDF score of each word in a document normalized by the Euclidean norm, then \n",
    "   $$tfidf(w,d)=\\frac{s(w,d)}{\\sqrt{\\sum_{w \\in d}{s(w,d)^2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.42804604, 0.        , 0.        ,\n",
       "         0.5628291 , 0.        , 0.5628291 , 0.42804604],\n",
       "        [0.        , 0.        , 0.59085245, 0.        , 0.        ,\n",
       "         0.38844998, 0.        , 0.38844998, 0.59085245],\n",
       "        [0.        , 0.52964479, 0.        , 0.74647284, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.40280852],\n",
       "        [0.51335285, 0.36423919, 0.27701329, 0.        , 0.51335285,\n",
       "         0.        , 0.51335285, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sklearn package - IDF.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                                   vocabulary=vocabulary,\n",
    "                                   smooth_idf=False)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).todense()\n",
    "print(tfidf_matrix.shape)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.42804604, 0.        , 0.        ,\n",
       "        0.5628291 , 0.        , 0.5628291 , 0.42804604],\n",
       "       [0.        , 0.        , 0.59085245, 0.        , 0.        ,\n",
       "        0.38844998, 0.        , 0.38844998, 0.59085245],\n",
       "       [0.        , 0.52964479, 0.        , 0.74647284, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.40280852],\n",
       "       [0.51335285, 0.36423919, 0.27701329, 0.        , 0.51335285,\n",
       "        0.        , 0.51335285, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simply multiply tf with idf and normalize the result.\n",
    "from sklearn.preprocessing import normalize\n",
    "normalize(np.array(tf_matrix)*idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.44493104, 0.        , 0.        ,\n",
       "         0.54957835, 0.        , 0.54957835, 0.44493104],\n",
       "        [0.        , 0.        , 0.60161783, 0.        , 0.        ,\n",
       "         0.37155886, 0.        , 0.37155886, 0.60161783],\n",
       "        [0.        , 0.55349232, 0.        , 0.70203482, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.44809973],\n",
       "        [0.49819711, 0.39278432, 0.31799276, 0.        , 0.49819711,\n",
       "         0.        , 0.49819711, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sklearn package - smoothed IDF.\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                                   vocabulary=vocabulary,\n",
    "                                   smooth_idf=True)\n",
    "smooth_tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).todense()\n",
    "print(smooth_tfidf_matrix.shape)\n",
    "smooth_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.44493104, 0.        , 0.        ,\n",
       "        0.54957835, 0.        , 0.54957835, 0.44493104],\n",
       "       [0.        , 0.        , 0.60161783, 0.        , 0.        ,\n",
       "        0.37155886, 0.        , 0.37155886, 0.60161783],\n",
       "       [0.        , 0.55349232, 0.        , 0.70203482, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.44809973],\n",
       "       [0.49819711, 0.39278432, 0.31799276, 0.        , 0.49819711,\n",
       "        0.        , 0.49819711, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simply multiply tf with smoothed idf and normalize the result.\n",
    "normalize(np.array(tf_matrix)*smoothed_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Document searching - find similar documents by similarity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cosine similarity\n",
    "- Given two vectors A and B:\n",
    "<img src='NLP/cosine_formula.svg' width='50%' />\n",
    "- Example 1 : A=[0,2,1], B=[0,2,2], then\n",
    "$$cosine(A,B)=\\frac{0*0+2*2+1*2}{\\sqrt{0+4+1}*\\sqrt{0+4+4}} = 1.6641$$\n",
    "\n",
    "- Example 2 : A=[0,2,1], B=[0,2,1], then\n",
    "$$cosine(A,B)=\\frac{0*0+2*2+1*1}{\\sqrt{0+4+1}*\\sqrt{0+4+1}} = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.943086  , 0.17242059, 0.11857444],\n",
       "       [0.943086  , 1.        , 0.2380004 , 0.16367398],\n",
       "       [0.17242059, 0.2380004 , 1.        , 0.19291739],\n",
       "       [0.11857444, 0.16367398, 0.19291739, 1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Package to measure 'distance'.\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Calculate cosine distance of every pair of documents.\n",
    "# similarity is equal to '1 - distance'.\n",
    "similarity = 1 - distance.squareform(distance.pdist(tfidf_matrix, 'cosine'))\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.943086]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine similarity: A dot product between two vectors. \n",
    "# Take first two vectors (documents) as an example.\n",
    "np.dot(tfidf_matrix[0, :], tfidf_matrix[1, :].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query document: Wise people think they are foolish.\n",
      "Similar document: Foolish foolish people think they are wise wise.\n",
      "Cosine similarity score: 0.9431\n",
      "\n",
      "Query document: Foolish foolish people think they are wise wise.\n",
      "Similar document: Wise people think they are foolish.\n",
      "Cosine similarity score: 0.9431\n",
      "\n",
      "Query document: I am definitely wise so this irritates me.\n",
      "Similar document: Foolish foolish people think they are wise wise.\n",
      "Cosine similarity score: 0.238\n",
      "\n",
      "Query document: ABC is for sure like definitely foolish.\n",
      "Similar document: I am definitely wise so this irritates me.\n",
      "Cosine similarity score: 0.1929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Document searching - find similar documents.\n",
    "for idx, doc in enumerate(corpus):\n",
    "    print('Query document: {}'.format(doc))\n",
    "    print('Similar document: {}'.format(corpus[np.argsort(similarity)[:,::-1][idx,1]]))\n",
    "    print('Cosine similarity score: {}'.format(round(similarity[idx,np.argsort(similarity)[:,::-1][idx,1]],4)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given two vectors q and p:\n",
    "<img src='Euclidean_distance.png' width=50% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.66261594, -0.28652976, -0.32772404],\n",
       "       [ 0.66261594,  1.        , -0.23450363, -0.29331049],\n",
       "       [-0.28652976, -0.23450363,  1.        , -0.27049802],\n",
       "       [-0.32772404, -0.29331049, -0.27049802,  1.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_similarity = 1 - distance.squareform(distance.pdist(tfidf_matrix, 'euclidean'))\n",
    "e_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626159359466615"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Euclidean distance: Root sum of square difference between two vectors.\n",
    "# similarity = 1 - Euclidean distance\n",
    "# Take first two vectors (documents) as an example.\n",
    "1 - (sum((np.array(tfidf_matrix[0, :].flatten() - tfidf_matrix[1, :]).flatten())**2)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query document: Wise people think they are foolish.\n",
      "Similar document: Foolish foolish people think they are wise wise.\n",
      "Euclidean similarity score: 0.6626\n",
      "\n",
      "Query document: Foolish foolish people think they are wise wise.\n",
      "Similar document: Wise people think they are foolish.\n",
      "Euclidean similarity score: 0.6626\n",
      "\n",
      "Query document: I am definitely wise so this irritates me.\n",
      "Similar document: Foolish foolish people think they are wise wise.\n",
      "Euclidean similarity score: -0.2345\n",
      "\n",
      "Query document: ABC is for sure like definitely foolish.\n",
      "Similar document: I am definitely wise so this irritates me.\n",
      "Euclidean similarity score: -0.2705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Document searching - find similar documents.\n",
    "for idx, doc in enumerate(corpus):\n",
    "    print('Query document: {}'.format(doc))\n",
    "    print('Similar document: {}'.format(corpus[np.argsort(e_similarity)[:,::-1][idx,1]]))\n",
    "    print('Euclidean similarity score: {}'.format(round(e_similarity[idx,np.argsort(e_similarity)[:,::-1][idx,1]],4)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
